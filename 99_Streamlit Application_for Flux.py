import streamlit as st
import pandas as pd
import os
import requests
import yt_dlp as yt_dlp
from googleapiclient.discovery import build
from openai import OpenAI
from openpyxl import load_workbook
import logging
import re 
from pydub import AudioSegment  # Audio splitting

# Initialize OpenAI client
# client = OpenAI(api_key=OPENAI_API_KEY)
log_file = "log.txt"
logging.basicConfig(filename=log_file, level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def log_message(message):
    """Logs and displays messages in Streamlit."""
    logging.info(message)
    st.write(message)

# Function to sanitize filenames
def sanitize_filename(file_name):
    file_name = str(file_name)  # Ensure it's a string
    file_name = file_name.replace("/", "").replace("â§¸", "")  # Remove only slashes
    return file_name

# Function to rename files inside a folder
def rename_files_in_folder(folder_path):
    if not os.path.exists(folder_path):
        print(f"Folder not found: {folder_path}")
        return

    for file in os.listdir(folder_path):
        old_file_path = os.path.join(folder_path, file)
        
        # Ensure it's a file (not a directory)
        if os.path.isfile(old_file_path):
            # Get file extension
            file_name, file_ext = os.path.splitext(file)
            
            # Sanitize only the filename (keep extension)
            sanitized_name = sanitize_filename(file_name)
            new_file_path = os.path.join(folder_path, f"{sanitized_name}{file_ext}")

            # Rename only if necessary
            if old_file_path != new_file_path:
                os.rename(old_file_path, new_file_path)
                print(f"Renamed: {old_file_path} â†’ {new_file_path}")
            else:
                print(f"No change needed: {old_file_path}")

# Function to extract Channel ID from YouTube link
def extract_channel_id(url):
    pattern = r"(?:youtube\.com/(?:channel/|user/|c/|@|.*[?&]channel_id=))([A-Za-z0-9_-]+)"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# Function to extract video ID from a YouTube URL
def extract_video_id(url):
    pattern = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# Function to get channel handle (username) from channel ID
def get_channel_handle(channel_id):
    url = f"https://www.googleapis.com/youtube/v3/channels?part=snippet,customUrl&id={channel_id}&key={YOUTUBE_API_KEY}"
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        if "items" in data and data["items"]:
            channel_info = data["items"][0]["snippet"]
            handle_name = channel_info.get("customUrl", "@Unknown")  # YouTube handle or "@Unknown"
            return handle_name
    return "@Unknown"

# Function to get detailed video information
def get_video_details(video_id):
    url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics,contentDetails,player,status,topicDetails,recordingDetails,liveStreamingDetails&id={video_id}&key={YOUTUBE_API_KEY}"
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        if "items" in data and data["items"]:
            video_info = data["items"][0]

            # Extract information from different sections
            snippet = video_info.get("snippet", {})
            statistics = video_info.get("statistics", {})
            content_details = video_info.get("contentDetails", {})
            player = video_info.get("player", {})
            status = video_info.get("status", {})
            topic_details = video_info.get("topicDetails", {})
            recording_details = video_info.get("recordingDetails", {})
            live_details = video_info.get("liveStreamingDetails", {})

            # Get Channel ID
            channel_id = snippet.get("channelId", "N/A")
            # Fetch Channel Handle Name
            handle_name = get_channel_handle(channel_id)

            # Convert topic IDs to a readable format
            topics = ", ".join(topic_details.get("topicIds", []))
            relevant_topics = ", ".join(topic_details.get("relevantTopicIds", []))
            topic_categories = ", ".join(topic_details.get("topicCategories", []))

            # Collect video data
            return {
                "Handle Name": handle_name,
                "Channel ID": channel_id,
                "Video ID": video_id,
                "Video Title": snippet.get("title", "N/A"),
                "Video Description": snippet.get("description", "N/A"),
                "Published Date": snippet.get("publishedAt", "N/A"),
                "Channel Name": snippet.get("channelTitle", "N/A"),
                "Category ID": snippet.get("categoryId", "N/A"),
                "Tags": ", ".join(snippet.get("tags", [])) if "tags" in snippet else "N/A",
                "Default Language": snippet.get("defaultLanguage", "N/A"),
                "Audio Language": snippet.get("defaultAudioLanguage", "N/A"),
                "Thumbnail URL": snippet.get("thumbnails", {}).get("high", {}).get("url", "N/A"),
                "View Count": statistics.get("viewCount", "N/A"),
                "Like Count": statistics.get("likeCount", "N/A"),
                "Comment Count": statistics.get("commentCount", "N/A"),
                "Video Duration": content_details.get("duration", "N/A"),
                "Video Quality": content_details.get("definition", "N/A"),
                "3D or 2D": content_details.get("dimension", "N/A"),
                "Captions Available": content_details.get("caption", "N/A"),
                "Licensed Content": content_details.get("licensedContent", "N/A"),
                "Projection Type": content_details.get("projection", "N/A"),
                "Embed HTML": player.get("embedHtml", "N/A"),
                "Privacy Status": status.get("privacyStatus", "N/A"),
                "Upload Status": status.get("uploadStatus", "N/A"),
                "Embeddable": status.get("embeddable", "N/A"),
                "Public Stats Viewable": status.get("publicStatsViewable", "N/A"),
                "Topic IDs": topics,
                "Relevant Topic IDs": relevant_topics,
                "Topic Categories": topic_categories,
                "Recording Date": recording_details.get("recordingDate", "N/A"),
                "Live Start Time": live_details.get("actualStartTime", "N/A"),
                "Live End Time": live_details.get("actualEndTime", "N/A"),
                "Scheduled Live Start": live_details.get("scheduledStartTime", "N/A"),
                "Scheduled Live End": live_details.get("scheduledEndTime", "N/A"),
                "Concurrent Viewers": live_details.get("concurrentViewers", "N/A"),
                "Live Chat ID": live_details.get("activeLiveChatId", "N/A"),
            }
    return None

# Function to format timestamps
def format_time_hms(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    remaining_seconds = seconds % 60
    return f"{hours:02}:{minutes:02}:{int(remaining_seconds):02}" if hours else f"{minutes:02}:{int(remaining_seconds):02}"

# Function to split audio into 60s chunks and return correct start times
def split_audio(file_path, chunk_length_ms=60000):  # Default: 60 seconds per chunk
    audio = AudioSegment.from_mp3(file_path)
    chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]
    
    chunk_paths = []
    chunk_start_times = []  # Store start times for timestamp correction
    base_name = os.path.splitext(file_path)[0]

    for i, chunk in enumerate(chunks):
        chunk_path = f"{base_name}_part{i}.mp3"
        chunk.export(chunk_path, format="mp3")
        chunk_paths.append(chunk_path)
        chunk_start_times.append(i * (chunk_length_ms / 1000))  # Convert ms to seconds
    
    return chunk_paths, chunk_start_times

###################################################
################### UI Creation ###################
###################################################
# Streamlit App Title
st.title("Fluxæ§˜ - YouTubeãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ãƒ¼ãƒ”ãƒ³ã‚°ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¢ãƒ—ãƒª")
st.subheader("ğŸ” API Keyå®šç¾©", divider=True)
OPENAI_API_KEY = st.text_input("OpenAI API Keyã‚’è¨˜å…¥ã—ã¦ãã ã•ã„", type="password")
YOUTUBE_API_KEY = st.text_input("YouTube API KeyKeyã‚’è¨˜å…¥ã—ã¦ãã ã•ã„", type="password")

# Only initialize OpenAI client if key is provided
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)
else:
    st.warning("OpenAI API Keyã‚’è¨˜å…¥ã—ã¦ãã ã•ã„")

# File Upload Section
st.subheader("ã‚¹ãƒ†ãƒƒãƒ—1ï¼šAnnalysisChannelæƒ…å ±ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š", divider=True)
st.markdown("ãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒ³ãƒ‰ãƒ«ã®ãƒªã‚¹ãƒˆãŒå«ã¾ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
save_folder = st.text_input("ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šC:/Users/YourName/Documents/YT_Dataï¼‰")
uploaded_file = st.file_uploader("YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒãƒ³ãƒ‰ãƒ«ãŒè¨˜è¼‰ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚(eg. @Hunter_Channel, @yo2_man, @metan-car-life)", type=["xlsx"])

if uploaded_file:    
    # Load the Excel file
    df = pd.read_excel(uploaded_file)
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—2ï¼šãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", divider=True)
    st.markdown("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹YouTubeãƒãƒ³ãƒ‰ãƒ«ã®ãƒªã‚¹ãƒˆãŒå«ã¾ã‚ŒãŸåˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    column_name = st.selectbox("YouTubeãƒãƒ³ãƒ‰ãƒ«ãŒå«ã¾ã‚Œã‚‹åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„", df.columns)
    ###########################################################################
    ################## Step 1: Channel Data Scraping ##########################
    ###########################################################################
    if st.button("ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ"):
        st.write("ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")

        youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
        channel_data = []

        for handle in df[column_name].dropna().astype(str).tolist():
            url = f"https://www.googleapis.com/youtube/v3/search?part=snippet&q={handle}&type=channel&key={YOUTUBE_API_KEY}"
            response = requests.get(url).json()

            if "items" in response and response["items"]:
                channel_id = response["items"][0]["id"]["channelId"]
            else:
                st.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒãƒ³ãƒ‰ãƒ«ã«å¯¾å¿œã™ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {handle}")
                continue

            request = youtube.channels().list(
                part="snippet,statistics,brandingSettings,contentDetails",
                id=channel_id
            )
            response = request.execute()

            if "items" in response:
                channel_info = response["items"][0]
                # Extract information from different sections
                snippet = channel_info.get("snippet", {})
                statistics = channel_info.get("statistics", {})
                branding = channel_info.get("brandingSettings", {})
                content_details = channel_info.get("contentDetails", {})
                topic_details = channel_info.get("topicDetails", {})
                localizations = channel_info.get("localizations", {})
                content_owner = channel_info.get("contentOwnerDetails", {})
                status = channel_info.get("status", {})

                # Convert topic IDs to string (list of topics)
                topics = ", ".join(topic_details.get("topicIds", []))
                relevant_topics = ", ".join(topic_details.get("relevantTopicIds", []))

                # Convert localizations to a readable format
                localization_info = "\n".join([
                    f"{lang}: {details.get('title', 'N/A')} - {details.get('description', 'N/A')}" 
                    for lang, details in localizations.items()
                ])

                # Collect all data
                channel_data.append({
                    "Handle": handle,
                    "Channel ID": channel_id,
                    "Channel Title": snippet.get("title", "N/A"),
                    "Channel Description": snippet.get("description", "N/A"),
                    "Published Date": snippet.get("publishedAt", "N/A"),
                    "Country": snippet.get("country", "N/A"),
                    "Subscribers": statistics.get("subscriberCount", "N/A"),
                    "Total Views": statistics.get("viewCount", "N/A"),
                    "Total Videos": statistics.get("videoCount", "N/A"),
                    "Custom URL": snippet.get("customUrl", "N/A"),
                    "Channel Keywords": branding.get("channel", {}).get("keywords", "N/A"),
                    "Analytics Tracking ID": branding.get("channel", {}).get("trackingAnalyticsAccountId", "N/A"),
                    "Trailer Video (Non-Subscribers)": branding.get("channel", {}).get("unsubscribedTrailer", "N/A"),
                    "Default Language": branding.get("channel", {}).get("defaultLanguage", "N/A"),
                    "Banner Image URL": branding.get("image", {}).get("bannerExternalUrl", "N/A"),
                    "Uploads Playlist ID": content_details.get("relatedPlaylists", {}).get("uploads", "N/A"),
                    "Likes Playlist ID": content_details.get("relatedPlaylists", {}).get("likes", "N/A"),
                    "Favorites Playlist ID": content_details.get("relatedPlaylists", {}).get("favorites", "N/A"),
                    "Watch Later Playlist ID": content_details.get("relatedPlaylists", {}).get("watchLater", "N/A"),
                    "Topic IDs": topics,
                    "Relevant Topic IDs": relevant_topics,
                    "Localization Info": localization_info,
                    "Content Owner": content_owner.get("contentOwner", "N/A"),
                    "Time Linked to Content Owner": content_owner.get("timeLinked", "N/A"),
                    "Privacy Status": status.get("privacyStatus", "N/A"),
                    "Is Linked to Google Account": status.get("isLinked", "N/A"),
                    "Long Uploads Status": status.get("longUploadsStatus", "N/A")
                })
            else:
                print(f"ãƒãƒ£ãƒ³ãƒãƒ«IDã®è©³ç´°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {channel_id}")

        channel_data_path = os.path.join(save_folder, "01_YouTube_Channel_Data.xlsx")
        pd.DataFrame(channel_data).to_excel(channel_data_path, index=False)
        st.success(f"ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã¯æ¬¡ã®å ´æ‰€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼š {channel_data_path}")
    ###########################################################################
    ################## Step 2: Video Data Scraping ############################
    ###########################################################################
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—3ï¼šå‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", divider=True)
    uploaded_file = st.file_uploader("YouTubeå‹•ç”»ãƒªãƒ³ã‚¯ãŒè¨˜è¼‰ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xlsx"])
    if uploaded_file:
        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        df = pd.read_excel(uploaded_file)
        column_name = st.selectbox("YouTubeãƒªãƒ³ã‚¯ãŒå«ã¾ã‚Œã‚‹åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„", df.columns)

        # ã‚¹ãƒ†ãƒƒãƒ—1ï¼šå‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if st.button("å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ"):
            st.write("å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            video_data = []
            youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)

            for _, row in df.iterrows():
                video_url = row[column_name]
                if pd.isna(video_url):
                    continue

                video_id = extract_video_id(video_url)
                if not video_id:
                    st.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‹ã‚‰å‹•ç”»IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {video_url}")
                    continue

                video_stats = get_video_details(video_id)
                if video_stats:
                    video_stats["Video URL"] = video_url  # Add original URL to results
                    video_data.append(video_stats)

            # ğŸ”¹ Save Data to Excel
            if save_folder:
                video_data_path = os.path.join(save_folder, "02_YouTube_Video_Data.xlsx")
                pd.DataFrame(video_data).to_excel(video_data_path, index=False)
                st.session_state.video_data_path = video_data_path
                st.success(f"âœ… å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã¯æ¬¡ã®å ´æ‰€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {video_data_path}")
            else:
                st.error("æœ‰åŠ¹ãªãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    ###########################################################################
    ################### Step 3: Download Videos & Audio #######################
    ###########################################################################
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—4ï¼šéŸ³å£°ãŠã‚ˆã³å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", divider=True)
    st.markdown("ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€MP4ãŠã‚ˆã³MP3ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”ŸæˆãŒå§‹ã¾ã‚Šã¾ã™ã€‚ã‚¹ãƒ†ãƒƒãƒ—1ã§æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã«æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã™ã€‚")

    # âœ… Ensure `video_data_path` exists before proceeding
    if st.button("MP4ã¨MP3ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
        if not save_folder:
            st.error("âŒ æœ‰åŠ¹ãªãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            log_message("âŒ ã‚¨ãƒ©ãƒ¼ï¼šä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            video_data_path = os.path.join(save_folder, "02_YouTube_Video_Data.xlsx")

            if not os.path.exists(video_data_path):
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{video_data_path}ã€‚å…ˆã«å‹•ç”»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                log_message(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼šå‹•ç”»ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{video_data_path}ï¼‰ã€‚")
            else:
                log_message("ğŸ“¥ å‹•ç”»ã¨éŸ³å£°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")

                video_df = pd.read_excel(video_data_path)
                video_ids = video_df["Video ID"].dropna().astype(str).tolist()
                youtube_links = [f"https://www.youtube.com/watch?v={video_id}" for video_id in video_ids]

                video_folder = os.path.join(save_folder, "Video")
                audio_folder = os.path.join(save_folder, "Audio")
                os.makedirs(video_folder, exist_ok=True)
                os.makedirs(audio_folder, exist_ok=True)

                log_message(f"â³ {len(youtube_links)} æœ¬ã®å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

                def video_hook(d):
                    if d['status'] == 'finished':
                        log_message(f"âœ… å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼š{d['filename']}")

                def audio_hook(d):
                    if d['status'] == 'finished':
                        log_message(f"ğŸµ éŸ³å£°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼š{d['filename']}")

                ydl_opts_video = {
                    'outtmpl': os.path.join(video_folder, f"{sanitize_filename('%(title)s')}.%(ext)s"), ##################
                    'format': 'bestvideo+bestaudio/best',
                    'progress_hooks': [video_hook]
                }
                ydl_opts_audio = {
                    'outtmpl': os.path.join(audio_folder, f"{sanitize_filename('%(title)s')}.%(ext)s"), ##################
                    'format': 'bestaudio/best',
                    'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}],
                    'progress_hooks': [audio_hook]
                }

                with yt_dlp.YoutubeDL(ydl_opts_video) as ydl:
                    ydl.download(youtube_links)
                log_message("âœ… ã™ã¹ã¦ã®å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

                with yt_dlp.YoutubeDL(ydl_opts_audio) as ydl:
                    ydl.download(youtube_links)
                log_message("âœ… ã™ã¹ã¦ã®éŸ³å£°ï¼ˆMP3ï¼‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                rename_files_in_folder(video_folder)
                rename_files_in_folder(audio_folder)
                st.session_state.save_folder = save_folder
                st.session_state.audio_folder = audio_folder
                st.session_state.video_folder = video_folder
                st.success(f"ğŸ‰ å‹•ç”»ã¯ {video_folder} ã«ã€MP3ã¯ {audio_folder} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    
##########################################
    # Step 5: Generating Transcripts
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—5ï¼šæ–‡å­—èµ·ã“ã—ã®ç”Ÿæˆ", divider=True)
    st.markdown("ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€**ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—èµ·ã“ã—** ã¨ **ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®æ–‡å­—èµ·ã“ã—** ã®2ç¨®é¡ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚")

    if st.button("æ–‡å­—èµ·ã“ã—ã‚’ç”Ÿæˆ"):
        if not save_folder:
            st.error("âŒ æœ‰åŠ¹ãªãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            audio_folder = os.path.join(save_folder, "Audio")
            transcript_folder = os.path.join(save_folder, "Transcript")
            timestamp_transcript_folder = os.path.join(save_folder, "Time Stamp Transcript")

            os.makedirs(transcript_folder, exist_ok=True)
            os.makedirs(timestamp_transcript_folder, exist_ok=True)

            if not os.path.exists(audio_folder):
                st.error(f"âŒ éŸ³å£°ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{audio_folder}ã€‚å…ˆã«MP3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.write("ğŸ“œ æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")

                # Loop through all MP3 files in the folder
                for file_name in os.listdir(audio_folder):
                    if file_name.endswith(".mp3"):
                        file_path = os.path.join(audio_folder, file_name)
                        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to MB
                        
                        st.write(f"â³ å‡¦ç†ä¸­ï¼š{file_name}ï¼ˆ{file_size_mb:.2f}MBï¼‰")

                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ25MBã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ†å‰²
                        if file_size_mb > 25:
                            st.write("ğŸ”¹ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã„ã¾ã™...")
                            chunk_files, chunk_start_times = split_audio(file_path)
                        else:
                            chunk_files = [file_path]
                            chunk_start_times = [0]  # Start from 0s if no split

                        transcript_text = []
                        data = []

                        # Process each audio chunk
                        for chunk_idx, chunk_file in enumerate(chunk_files):
                            chunk_start_time = chunk_start_times[chunk_idx]

                            with open(chunk_file, "rb") as audio_chunk:
                                try:
                                    transcription_data = client.audio.transcriptions.create(
                                        model="whisper-1",
                                        file=audio_chunk,
                                        response_format="verbose_json"
                                    )
                                    transcription_data = transcription_data.model_dump()
                                except Exception as e:
                                    st.error(f"âŒ {chunk_file} ã®æ–‡å­—èµ·ã“ã—ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¾ã—ãŸï¼š{e}")
                                    continue

                            # Extract transcript data
                            if "segments" in transcription_data:
                                for segment in transcription_data["segments"]:
                                    start_time = format_time_hms(segment.get("start", 0) + chunk_start_time)
                                    end_time = format_time_hms(segment.get("end", 0) + chunk_start_time)
                                    text = str(segment.get("text", "N/A"))  # Ensure text is a string

                                    # Handle NaN values
                                    start_time = start_time if start_time else "00:00"
                                    end_time = end_time if end_time else "00:00"

                                    data.append([start_time, end_time, text])
                                    transcript_text.append(text)


                            # Remove chunk files after processing
                            os.remove(chunk_file)

                        # Convert to DataFrame
                        df = pd.DataFrame(data, columns=["Start Time", "End Time", "Text"])

                        # Define output file names
                        base_name = os.path.splitext(file_name)[0]
                        excel_file_path = os.path.join(timestamp_transcript_folder, f"{base_name}_timestamp_transcript.xlsx")
                        text_file_path = os.path.join(transcript_folder, f"{base_name}_transcript.txt")

                        # Save timestamped transcript as an Excel file
                        df.to_excel(excel_file_path, index=False, engine='openpyxl')

                        # Adjust column widths in Excel
                        wb = load_workbook(excel_file_path)
                        ws = wb.active
                        ws.column_dimensions['A'].width = 50 / 7
                        ws.column_dimensions['B'].width = 50 / 7
                        ws.column_dimensions['C'].width = 500 / 7
                        wb.save(excel_file_path)

                        st.write(f"ğŸ“„ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãæ–‡å­—èµ·ã“ã—ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{excel_file_path}")

                # ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—èµ·ã“ã—ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                with open(text_file_path, "w", encoding="utf-8") as txt_file:
                    for text in transcript_text:
                        txt_file.write(text + "\n")

                st.write(f"ğŸ“„ ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—èµ·ã“ã—ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{text_file_path}")

                st.success(f"ğŸ“œ æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {transcript_folder} ã¨ {timestamp_transcript_folder} ã«ä¿å­˜ã—ã¾ã—ãŸ")
                
    ###########################################################################
    #########################  âœ… Step 7: Show Logs ##########################
    ###########################################################################
    if st.button("ãƒ­ã‚°ã‚’è¡¨ç¤º"):
        if os.path.exists(log_file):
            with open(log_file, "r") as f:
                logs = f.read()
            st.text_area("ğŸ“„ ãƒ­ã‚°å‡ºåŠ›ï¼š", logs, height=300)
        else:
            st.warning("âš  ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
